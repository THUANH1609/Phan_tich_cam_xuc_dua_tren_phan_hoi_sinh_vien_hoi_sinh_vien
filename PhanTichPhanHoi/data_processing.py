import torch
import re
import os
from transformers import AutoTokenizer, AutoModel
from model_classes import PhoBERT_CNN_GRU_Sentiment, PhoBERT_GRU_Topic # C·∫ßn ƒë·∫£m b·∫£o file n√†y t·ªìn t·∫°i

# ----------------------------------------------------
# 1. THI·∫æT L·∫¨P V√Ä LOAD MODEL (Ch·ªâ ch·∫°y 1 l·∫ßn)
# ----------------------------------------------------
MAX_LEN = 96
# üö® QUAN TR·ªåNG: Bu·ªôc ch·∫°y tr√™n CPU (ho·∫∑c GPU n·∫øu c√≥)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') 
print(f"Loading models on device: {device}")
VERBOSE = False

# Load Tokenizer v√† PhoBERT Base
try:
    tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
    phobert_base = AutoModel.from_pretrained("vinai/phobert-base")
except Exception as e:
    print(f"‚ùå L·ªñI KH·ªûI T·∫†O: Kh√¥ng th·ªÉ t·∫£i PhoBERT/Tokenizer. {e}")
    raise

# Kh·ªüi t·∫°o c·∫•u tr√∫c m√¥ h√¨nh
model_sent = PhoBERT_CNN_GRU_Sentiment(phobert_base, n_classes=3)
model_topic = PhoBERT_GRU_Topic(phobert_base, n_classes=4)

# Load tr·ªçng s·ªë ƒë√£ l∆∞u
MODEL_PATH = os.path.join(os.path.dirname(__file__), 'models') 

try:
    # üö® D√πng map_location ƒë·ªÉ ƒë·∫£m b·∫£o t·∫£i ƒë√∫ng thi·∫øt b·ªã (CPU ho·∫∑c CUDA)
    model_sent.load_state_dict(torch.load(
        os.path.join(MODEL_PATH, 'sent_phobert_hybrid_best.pth'), 
        map_location=device
    ))
    model_topic.load_state_dict(torch.load(
        os.path.join(MODEL_PATH, 'topic_phobert_gru_best.pth'), 
        map_location=device
    ))
    
    # Chuy·ªÉn m√¥ h√¨nh sang thi·∫øt b·ªã v√† ƒë·∫∑t ·ªü ch·∫ø ƒë·ªô ƒë√°nh gi√°
    model_sent.to(device).eval()
    model_topic.to(device).eval()
    print("‚úÖ Model weights loaded and models set to evaluation mode.")

except Exception as e:
    print(f"‚ùå L·ªñI LOAD TR·ªåNG S·ªê: Ki·ªÉm tra th∆∞ m·ª•c 'models/' v√† file .pth. {e}")
    # ƒê∆∞a ra l·ªói ƒë·ªÉ Uvicorn hi·ªÉn th·ªã Traceback chi ti·∫øt
    raise

# ƒê·ªãnh nghƒ©a c√°c √°nh x·∫° nh√£n (ƒë√£ ki·ªÉm tra v√† s·ª≠a l·ªói)
sentiment_map = {0:"üò° Ti√™u c·ª±c", 1:"üòê Trung l·∫≠p", 2:"üòä T√≠ch c·ª±c"}
topic_map = {0:"üßë‚Äçüè´ Gi·∫£ng vi√™n", 1:"üìò Ch∆∞∆°ng tr√¨nh h·ªçc", 2:"üè´ C∆° s·ªü v·∫≠t ch·∫•t", 3:"üíª H·ªçc li·ªáu/Website"}


# ----------------------------------------------------
# 2. H√ÄM X·ª¨ L√ù D·ªÆ LI·ªÜU
# ----------------------------------------------------

# H√†m T√°ch C√¢u theo t·ª´ n·ªëi v√† d·∫•u c√¢u
def split_feedback_text(text):
    """
    T√°ch vƒÉn b·∫£n th√†nh c√°c ph·∫ßn d·ª±a tr√™n:
    - T·ª´ n·ªëi ƒë·ªëi l·∫≠p: nh∆∞ng, tuy nhi√™n, c√≤n, song
    - T·ª´ n·ªëi b·ªï sung: v√†, c√≤n
    - D·∫•u c√¢u: . ! ?
    
    V√≠ d·ª•: "th·∫ßy d·∫°y hay, nh∆∞ng c∆° s·ªü v·∫≠t ch·∫•t k√©m" 
           ‚Üí ["th·∫ßy d·∫°y hay", "c∆° s·ªü v·∫≠t ch·∫•t k√©m"]
    """
    text = text.strip()
    
    # T√°ch theo d·∫•u c√¢u tr∆∞·ªõc
    sentences = re.split(r'[.!?]+', text)
    
    all_parts = []
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # T√°ch m·ªói c√¢u theo t·ª´ n·ªëi (b·ªè d·∫•u ph·∫©y tr∆∞·ªõc t·ª´ n·ªëi)
        parts = re.split(
            r',?\s*(?:\bnh∆∞ng m√†\b|\bnh∆∞ng\b|\btuy nhi√™n\b|\bm√†\b|\bc√≤n\b|\bsong\b|\bv√†\b)', 
            sentence, 
            flags=re.IGNORECASE
        )
        
        # L·ªçc v√† l√†m s·∫°ch c√°c ph·∫ßn
        cleaned_parts = [p.strip(" ,.") for p in parts if p.strip()]
        all_parts.extend(cleaned_parts)
    
    return all_parts if all_parts else [text]

# H√†m D·ª± ƒëo√°n Ch√≠nh
def preprocess_text(text):
    """Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n tr∆∞·ªõc khi ƒë∆∞a v√†o m√¥ h√¨nh"""
    # X√≥a c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt
    text = re.sub(r'[^\w\s√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√ö√ù√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫√ΩƒÇƒÉƒêƒëƒ®ƒ©≈®≈©∆†∆°∆Ø∆∞·∫†-·ªπ]', ' ', text)
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def predict_feedback(text):
    """
    D·ª± ƒëo√°n c·∫£m x√∫c v√† ch·ªß ƒë·ªÅ cho vƒÉn b·∫£n ƒë·∫ßu v√†o
    
    Args:
        text (str): VƒÉn b·∫£n c·∫ßn ph√¢n t√≠ch
        
    Returns:
        tuple: (sentiment, topic, confidence_sent, confidence_topic)
    """
    try:
        # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
        text = preprocess_text(text)
        if not text:
            return "Kh√¥ng x√°c ƒë·ªãnh", "Kh√¥ng x√°c ƒë·ªãnh", 0.0, 0.0
            
        # M√£ h√≥a vƒÉn b·∫£n
        enc = tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=MAX_LEN,
            return_tensors='pt'
        )
        
        # Chuy·ªÉn d·ªØ li·ªáu v√†o device ph√π h·ª£p
        ids = enc['input_ids'].to(device)
        mask = enc['attention_mask'].to(device)
        
        # D·ª± ƒëo√°n
        with torch.no_grad():
            model_sent.eval()
            model_topic.eval()
            
            # D·ª± ƒëo√°n c·∫£m x√∫c
            s_out = model_sent(ids, mask)
            s_probs = torch.softmax(s_out, dim=1)
            s_label = torch.argmax(s_probs, dim=1).item()
            s_confidence = s_probs[0][s_label].item()
            
            # D·ª± ƒëo√°n ch·ªß ƒë·ªÅ
            t_out = model_topic(ids, mask)
            t_probs = torch.softmax(t_out, dim=1)
            t_label = torch.argmax(t_probs, dim=1).item()
            t_confidence = t_probs[0][t_label].item()
            
            # L·∫•y nh√£n t∆∞∆°ng ·ª©ng
            sentiment = sentiment_map.get(s_label, "Kh√¥ng x√°c ƒë·ªãnh")
            topic = topic_map.get(t_label, "Kh√¥ng x√°c ƒë·ªãnh")

        # -----------------------------
        # L·ªõp s·ª≠a lu·∫≠t (rule-based) nh·∫π
        # -----------------------------
        txt_lower = text.lower()
        pos_words = [
            "hay", "t·ªët", "tuy·ªát v·ªùi", "h√†i l√≤ng", "nhi·ªát t√¨nh", "th√¢n thi·ªán",
            "d·ªÖ hi·ªÉu", "·ªïn", "ok", "ƒë·∫πp", "s·∫°ch", "ch·∫•t l∆∞·ª£ng", "t·∫≠n t√¢m"
        ]
        neg_words = [
            "t·ªá", "k√©m", "k√©m ch·∫•t l∆∞·ª£ng", "ƒë·∫Øt", "ch√°n", "·ªìn", "b·∫©n",
            "c≈©", "kh√≥ hi·ªÉu", "ch·∫≠m", "l√¢u", "qu√° t·∫£i", "thi·∫øu"
        ]
        teacher_words = [
            "th·∫ßy", "c√¥", "gi·∫£ng vi√™n", "gv", "d·∫°y", "gi·∫£ng d·∫°y", 
            "gvcn", "ch·ªß nhi·ªám", "gi√°o vi√™n", "th·∫ßy c√¥", "ƒë·ªôi ng≈© gi·∫£ng vi√™n",
            "ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y", "k·ªπ nƒÉng s∆∞ ph·∫°m", "nhi·ªát t√¨nh"
        ]
        facility_words = [
            "c∆° s·ªü v·∫≠t ch·∫•t", "ph√≤ng h·ªçc", "b√†n gh·∫ø", "wifi", "m√°y chi·∫øu", 
            "ph√≤ng", "csdl", "th∆∞ vi·ªán", "ph√≤ng th√≠ nghi·ªám", "ph√≤ng lab",
            "k√Ω t√∫c x√°", "ktx", "cƒÉn tin", "s√¢n ch∆°i", "thi·∫øt b·ªã", "trang thi·∫øt b·ªã"
        ]
        program_words = [
            "ch∆∞∆°ng tr√¨nh", "h·ªçc ph·∫ßn", "m√¥n", "khung ch∆∞∆°ng tr√¨nh", 
            "t√≠n ch·ªâ", "l·ªãch h·ªçc", "ch·∫•t l∆∞·ª£ng ƒë√†o t·∫°o", "n·ªôi dung h·ªçc", 
            "ki·∫øn th·ª©c", "k·ªπ nƒÉng", "th·ª±c h√†nh", "l√Ω thuy·∫øt", "b√†i t·∫≠p"
        ]
        material_words = [
            "h·ªçc li·ªáu", "t√†i li·ªáu", "website", "web", "lms", "moodle", 
            "b√†i gi·∫£ng", "slides", "gi√°o tr√¨nh", "s√°ch", "t∆∞ li·ªáu",
            "h·ªçc ph√≠", "chi ph√≠", "l·ªá ph√≠", "ti·ªÅn h·ªçc", "ƒë√≥ng h·ªçc"
        ]

        def contains_any(words):
            return any(w in txt_lower for w in words)

        # X·ª≠ l√Ω ph·ªß ƒë·ªãnh ƒë∆°n gi·∫£n: "kh√¥ng/ch∆∞a/ch·∫≥ng + t√≠ch c·ª±c" => ti√™u c·ª±c
        negation_tokens = ["kh√¥ng", "ch∆∞a", "ch·∫≥ng", "ch·∫£"]
        has_negation = any(tok in txt_lower for tok in negation_tokens)
        pos_hit = contains_any(pos_words)
        neg_hit = contains_any(neg_words)

        # S·ª≠a c·∫£m x√∫c CH·ªà KHI m√¥ h√¨nh kh√¥ng ch·∫Øc ch·∫Øn (trung l·∫≠p ho·∫∑c confidence th·∫•p)
        if sentiment == "üòê Trung l·∫≠p" or s_confidence < 0.6:
            # Ph·ªß ƒë·ªãnh + t√≠ch c·ª±c ‚Üí ti√™u c·ª±c (v√≠ d·ª•: "kh√¥ng hay")
            if has_negation and pos_hit and not neg_hit:
                sentiment = "üò° Ti√™u c·ª±c"
                s_confidence = 0.75
            # Ch·ªâ c√≥ t·ª´ ti√™u c·ª±c ‚Üí ti√™u c·ª±c
            elif neg_hit and not pos_hit:
                sentiment = "üò° Ti√™u c·ª±c"
                s_confidence = 0.75
            # Ch·ªâ c√≥ t·ª´ t√≠ch c·ª±c ‚Üí t√≠ch c·ª±c
            elif pos_hit and not neg_hit and not has_negation:
                sentiment = "üòä T√≠ch c·ª±c"
                s_confidence = 0.75

        # S·ª≠a ch·ªß ƒë·ªÅ b·∫±ng ch·ªâ b√°o t·ª´ kho√° r√µ r√†ng
        if contains_any(teacher_words):
            topic = topic_map.get(0, topic)  # Gi·∫£ng vi√™n
            t_confidence = max(t_confidence, 0.75)
        elif contains_any(program_words):
            topic = topic_map.get(1, topic)  # Ch∆∞∆°ng tr√¨nh
            t_confidence = max(t_confidence, 0.75)
        elif contains_any(facility_words):
            topic = topic_map.get(2, topic)  # C∆° s·ªü v·∫≠t ch·∫•t
            t_confidence = max(t_confidence, 0.75)
        elif contains_any(material_words):
            topic = topic_map.get(3, topic)  # H·ªçc li·ªáu/Website
            t_confidence = max(t_confidence, 0.75)
        
        # In th√¥ng tin debug
        if VERBOSE:
            print(f"\nüìù VƒÉn b·∫£n: {text}")
            print(f"üòä C·∫£m x√∫c: {sentiment} (ƒê·ªô tin c·∫≠y: {s_confidence*100:.1f}%)")
            print(f"üè∑Ô∏è Ch·ªß ƒë·ªÅ: {topic} (ƒê·ªô tin c·∫≠y: {t_confidence*100:.1f}%)")
        
        return sentiment, topic, s_confidence, t_confidence
            
    except Exception as e:
        print(f"‚ùå L·ªói khi d·ª± ƒëo√°n: {str(e)}")
        return "L·ªói", "L·ªói", 0.0, 0.0

# H√†m x·ª≠ l√Ω ph√¢n t√≠ch to√†n b·ªô ph·∫£n h·ªìi
def analyze_feedback_text(full_text):
    """
    Ph√¢n t√≠ch to√†n b·ªô vƒÉn b·∫£n ph·∫£n h·ªìi, t√°ch th√†nh c√°c c√¢u v√† ph√¢n t√≠ch t·ª´ng c√¢u
    
    Args:
        full_text (str): To√†n b·ªô vƒÉn b·∫£n ph·∫£n h·ªìi
        
    Returns:
        list: Danh s√°ch k·∫øt qu·∫£ ph√¢n t√≠ch cho t·ª´ng c√¢u
    """
    # T√°ch c√¢u d·ª±a tr√™n t·ª´ n·ªëi v√† d·∫•u c√¢u ƒë·ªÉ x·ª≠ l√Ω c√°c ƒë√°nh gi√° tr√°i ng∆∞·ª£c
    # V√≠ d·ª•: "th·∫ßy d·∫°y hay, nh∆∞ng c∆° s·ªü v·∫≠t ch·∫•t k√©m" ‚Üí ["th·∫ßy d·∫°y hay", "c∆° s·ªü v·∫≠t ch·∫•t k√©m"]
    sentences = split_feedback_text(full_text)

    # Batch tokenize ƒë·ªÉ tƒÉng t·ªëc
    cleaned = [preprocess_text(s) for s in sentences]
    if not cleaned:
        cleaned = [preprocess_text(full_text)]
    enc = tokenizer(
        cleaned,
        truncation=True,
        padding='max_length',
        max_length=MAX_LEN,
        return_tensors='pt'
    )
    ids = enc['input_ids'].to(device)
    mask = enc['attention_mask'].to(device)

    with torch.no_grad():
        model_sent.eval()
        model_topic.eval()
        s_out = model_sent(ids, mask)
        t_out = model_topic(ids, mask)
        s_probs = torch.softmax(s_out, dim=1)
        t_probs = torch.softmax(t_out, dim=1)
        s_labels = torch.argmax(s_probs, dim=1)
        t_labels = torch.argmax(t_probs, dim=1)
        s_conf = s_probs.gather(1, s_labels.view(-1,1)).squeeze(1)
        t_conf = t_probs.gather(1, t_labels.view(-1,1)).squeeze(1)

    results = []
    for i, sentence in enumerate(cleaned, 1):
        s_label = int(s_labels[i-1].item())
        t_label = int(t_labels[i-1].item())
        s_confidence = float(s_conf[i-1].item())
        t_confidence = float(t_conf[i-1].item())
        sentiment = sentiment_map.get(s_label, "Kh√¥ng x√°c ƒë·ªãnh")
        topic = topic_map.get(t_label, "Kh√¥ng x√°c ƒë·ªãnh")

        # Rule-based ƒëi·ªÅu ch·ªânh nh·∫π (gi·ªØ nguy√™n nh∆∞ predict_feedback)
        txt_lower = sentence.lower()
        pos_words = [
            "hay", "t·ªët", "tuy·ªát v·ªùi", "h√†i l√≤ng", "nhi·ªát t√¨nh", "th√¢n thi·ªán",
            "d·ªÖ hi·ªÉu", "·ªïn", "ok", "ƒë·∫πp", "s·∫°ch", "ch·∫•t l∆∞·ª£ng", "t·∫≠n t√¢m"
        ]
        neg_words = [
            "t·ªá", "k√©m", "k√©m ch·∫•t l∆∞·ª£ng", "ƒë·∫Øt", "ch√°n", "·ªìn", "b·∫©n",
            "c≈©", "kh√≥ hi·ªÉu", "ch·∫≠m", "l√¢u", "qu√° t·∫£i", "thi·∫øu"
        ]
        teacher_words = [
            "th·∫ßy", "c√¥", "gi·∫£ng vi√™n", "gv", "d·∫°y", "gi·∫£ng d·∫°y", 
            "gvcn", "ch·ªß nhi·ªám", "gi√°o vi√™n", "th·∫ßy c√¥", "ƒë·ªôi ng≈© gi·∫£ng vi√™n",
            "ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y", "k·ªπ nƒÉng s∆∞ ph·∫°m", "nhi·ªát t√¨nh"
        ]
        facility_words = [
            "c∆° s·ªü v·∫≠t ch·∫•t", "ph√≤ng h·ªçc", "b√†n gh·∫ø", "wifi", "m√°y chi·∫øu", 
            "ph√≤ng", "csdl", "th∆∞ vi·ªán", "ph√≤ng th√≠ nghi·ªám", "ph√≤ng lab",
            "k√Ω t√∫c x√°", "ktx", "cƒÉn tin", "s√¢n ch∆°i", "thi·∫øt b·ªã", "trang thi·∫øt b·ªã"
        ]
        program_words = [
            "ch∆∞∆°ng tr√¨nh", "h·ªçc ph·∫ßn", "m√¥n", "khung ch∆∞∆°ng tr√¨nh", 
            "t√≠n ch·ªâ", "l·ªãch h·ªçc", "ch·∫•t l∆∞·ª£ng ƒë√†o t·∫°o", "n·ªôi dung h·ªçc", 
            "ki·∫øn th·ª©c", "k·ªπ nƒÉng", "th·ª±c h√†nh", "l√Ω thuy·∫øt", "b√†i t·∫≠p"
        ]
        material_words = [
            "h·ªçc li·ªáu", "t√†i li·ªáu", "website", "web", "lms", "moodle", 
            "b√†i gi·∫£ng", "slides", "gi√°o tr√¨nh", "s√°ch", "t∆∞ li·ªáu",
            "h·ªçc ph√≠", "chi ph√≠", "l·ªá ph√≠", "ti·ªÅn h·ªçc", "ƒë√≥ng h·ªçc"
        ]
        def contains_any(words):
            return any(w in txt_lower for w in words)
        negation_tokens = ["kh√¥ng", "ch∆∞a", "ch·∫≥ng", "ch·∫£"]
        has_negation = any(tok in txt_lower for tok in negation_tokens)
        pos_hit = contains_any(pos_words)
        neg_hit = contains_any(neg_words)
        if sentiment == "üòê Trung l·∫≠p" or s_confidence < 0.6:
            if has_negation and pos_hit and not neg_hit:
                sentiment = "üò° Ti√™u c·ª±c"; s_confidence = 0.75
            elif neg_hit and not pos_hit:
                sentiment = "üò° Ti√™u c·ª±c"; s_confidence = 0.75
            elif pos_hit and not neg_hit and not has_negation:
                sentiment = "üòä T√≠ch c·ª±c"; s_confidence = 0.75
        if contains_any(teacher_words):
            topic = topic_map.get(0, topic); t_confidence = max(t_confidence, 0.75)
        elif contains_any(program_words):
            topic = topic_map.get(1, topic); t_confidence = max(t_confidence, 0.75)
        elif contains_any(facility_words):
            topic = topic_map.get(2, topic); t_confidence = max(t_confidence, 0.75)
        elif contains_any(material_words):
            topic = topic_map.get(3, topic); t_confidence = max(t_confidence, 0.75)

        results.append({
            'sentence_id': i,
            'text': sentences[i-1],
            'sentiment': sentiment,
            'topic': topic,
            'sentiment_confidence': round(float(s_confidence) * 100, 1),
            'topic_confidence': round(float(t_confidence) * 100, 1)
        })
    
    # In k·∫øt qu·∫£ t·ªïng h·ª£p
    if VERBOSE:
        print("\nüìä K·∫æT QU·∫¢ PH√ÇN T√çCH CHI TI·∫æT:")
        for i, result in enumerate(results, 1):
            print(f"\nüîç C√¢u {i}:")
            print(f"   - N·ªôi dung: {result['text']}")
            print(f"   - C·∫£m x√∫c: {result['sentiment']} ({result['sentiment_confidence']}%)")
            print(f"   - Ch·ªß ƒë·ªÅ: {result['topic']} ({result['topic_confidence']}%)")
    
    return results

# ----------------------------------------------------
# Ph√¢n t√≠ch nhi·ªÅu ph·∫£n h·ªìi h√†ng lo·∫°t (t·ªëi ∆∞u cho /analyze_file)
# ----------------------------------------------------
def analyze_many_texts(text_list, batch_size: int = 64):
    """Ph√¢n t√≠ch h√†ng lo·∫°t nhi·ªÅu ph·∫£n h·ªìi.
    Tr·∫£ v·ªÅ: List[List[dict]] t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng feedback ban ƒë·∫ßu.
    """
    # 1) T√°ch c√¢u cho t·ª´ng feedback v√† flatten
    per_text_sentences = [split_feedback_text(t or '') for t in text_list]
    flat_sentences = []
    owners = []  # (text_idx, local_sentence_id)
    for idx, sents in enumerate(per_text_sentences):
        if not sents:
            continue
        for j, s in enumerate(sents, 1):
            flat_sentences.append(preprocess_text(s))
            owners.append((idx, j))

    if not flat_sentences:
        return [[] for _ in text_list]

    # 2) Ch·∫°y m√¥ h√¨nh theo l√¥
    all_results = [None] * len(flat_sentences)
    for start in range(0, len(flat_sentences), batch_size):
        chunk = flat_sentences[start:start+batch_size]
        enc = tokenizer(
            chunk,
            truncation=True,
            padding='max_length',
            max_length=MAX_LEN,
            return_tensors='pt'
        )
        ids = enc['input_ids'].to(device)
        mask = enc['attention_mask'].to(device)
        with torch.no_grad():
            model_sent.eval(); model_topic.eval()
            s_out = model_sent(ids, mask)
            t_out = model_topic(ids, mask)
            s_probs = torch.softmax(s_out, dim=1)
            t_probs = torch.softmax(t_out, dim=1)
            s_labels = torch.argmax(s_probs, dim=1)
            t_labels = torch.argmax(t_probs, dim=1)
            s_conf = s_probs.gather(1, s_labels.view(-1,1)).squeeze(1)
            t_conf = t_probs.gather(1, t_labels.view(-1,1)).squeeze(1)

        for i in range(len(chunk)):
            global_idx = start + i
            s_label = int(s_labels[i].item()); t_label = int(t_labels[i].item())
            s_confidence = float(s_conf[i].item()); t_confidence = float(t_conf[i].item())
            sentiment = sentiment_map.get(s_label, "Kh√¥ng x√°c ƒë·ªãnh")
            topic = topic_map.get(t_label, "Kh√¥ng x√°c ƒë·ªãnh")

            # Rule-based ƒëi·ªÅu ch·ªânh nh·∫π (gi·ªëng ·ªü tr√™n)
            txt_lower = chunk[i].lower()
            pos_words = [
                "hay", "t·ªët", "tuy·ªát v·ªùi", "h√†i l√≤ng", "nhi·ªát t√¨nh", "th√¢n thi·ªán",
                "d·ªÖ hi·ªÉu", "·ªïn", "ok", "ƒë·∫πp", "s·∫°ch", "ch·∫•t l∆∞·ª£ng", "t·∫≠n t√¢m"
            ]
            neg_words = [
                "t·ªá", "k√©m", "k√©m ch·∫•t l∆∞·ª£ng", "ƒë·∫Øt", "ch√°n", "·ªìn", "b·∫©n",
                "c≈©", "kh√≥ hi·ªÉu", "ch·∫≠m", "l√¢u", "qu√° t·∫£i", "thi·∫øu"
            ]
            teacher_words = [
                "th·∫ßy", "c√¥", "gi·∫£ng vi√™n", "gv", "d·∫°y", "gi·∫£ng d·∫°y", 
                "gvcn", "ch·ªß nhi·ªám", "gi√°o vi√™n", "th·∫ßy c√¥", "ƒë·ªôi ng≈© gi·∫£ng vi√™n",
                "ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y", "k·ªπ nƒÉng s∆∞ ph·∫°m", "nhi·ªát t√¨nh"
            ]
            facility_words = [
                "c∆° s·ªü v·∫≠t ch·∫•t", "ph√≤ng h·ªçc", "b√†n gh·∫ø", "wifi", "m√°y chi·∫øu", 
                "ph√≤ng", "csdl", "th∆∞ vi·ªán", "ph√≤ng th√≠ nghi·ªám", "ph√≤ng lab",
                "k√Ω t√∫c x√°", "ktx", "cƒÉn tin", "s√¢n ch∆°i", "thi·∫øt b·ªã", "trang thi·∫øt b·ªã"
            ]
            program_words = [
                "ch∆∞∆°ng tr√¨nh", "h·ªçc ph·∫ßn", "m√¥n", "khung ch∆∞∆°ng tr√¨nh", 
                "t√≠n ch·ªâ", "l·ªãch h·ªçc", "ch·∫•t l∆∞·ª£ng ƒë√†o t·∫°o", "n·ªôi dung h·ªçc", 
                "ki·∫øn th·ª©c", "k·ªπ nƒÉng", "th·ª±c h√†nh", "l√Ω thuy·∫øt", "b√†i t·∫≠p"
            ]
            material_words = [
                "h·ªçc li·ªáu", "t√†i li·ªáu", "website", "web", "lms", "moodle", 
                "b√†i gi·∫£ng", "slides", "gi√°o tr√¨nh", "s√°ch", "t∆∞ li·ªáu",
                "h·ªçc ph√≠", "chi ph√≠", "l·ªá ph√≠", "ti·ªÅn h·ªçc", "ƒë√≥ng h·ªçc"
            ]
            def contains_any(words):
                return any(w in txt_lower for w in words)
            negation_tokens = ["kh√¥ng", "ch∆∞a", "ch·∫≥ng", "ch·∫£"]
            has_negation = any(tok in txt_lower for tok in negation_tokens)
            pos_hit = contains_any(pos_words); neg_hit = contains_any(neg_words)
            if sentiment == "üòê Trung l·∫≠p" or s_confidence < 0.6:
                if has_negation and pos_hit and not neg_hit:
                    sentiment = "üò° Ti√™u c·ª±c"; s_confidence = 0.75
                elif neg_hit and not pos_hit:
                    sentiment = "üò° Ti√™u c·ª±c"; s_confidence = 0.75
                elif pos_hit and not neg_hit and not has_negation:
                    sentiment = "üòä T√≠ch c·ª±c"; s_confidence = 0.75
            if contains_any(teacher_words):
                topic = topic_map.get(0, topic); t_confidence = max(t_confidence, 0.75)
            elif contains_any(program_words):
                topic = topic_map.get(1, topic); t_confidence = max(t_confidence, 0.75)
            elif contains_any(facility_words):
                topic = topic_map.get(2, topic); t_confidence = max(t_confidence, 0.75)
            elif contains_any(material_words):
                topic = topic_map.get(3, topic); t_confidence = max(t_confidence, 0.75)

            all_results[global_idx] = {
                'text': chunk[i],
                'sentiment': sentiment,
                'topic': topic,
                'sentiment_confidence': round(float(s_confidence) * 100, 1),
                'topic_confidence': round(float(t_confidence) * 100, 1)
            }

    # 3) Gom l·∫°i theo feedback ban ƒë·∫ßu
    grouped = [[] for _ in text_list]
    for (owner_idx, local_id), res in zip(owners, all_results):
        grouped[owner_idx].append(res)
    return grouped